{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -e gym-TicTacToe/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_TicTacToe\n",
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from agent import TicTacToeAgent\n",
    "import tqdm\n",
    "\n",
    "# initialize the tictactoe environment\n",
    "env = gym.envs.make(\"TTT-v0\", small=-1, large=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== Training resume ======================\n",
      "Episode 20000, average reward: -18.21\n",
      "Perc. wins:  10.16\n",
      "Epsilon: 0.9328\n",
      "Len QTable: 3296\n",
      "=============================================================\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 21995/2500000 [00:29<56:07, 735.91it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m     steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     36\u001b[0m ep_rewards\u001b[38;5;241m.\u001b[39mappend(sum_reward)\n\u001b[1;32m---> 37\u001b[0m epl_avg\u001b[38;5;241m.\u001b[39mappend(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mep_rewards\u001b[49m\u001b[43m[\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mep\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mtalk_every\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (ep\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m talk_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     40\u001b[0m     clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\repositories\\polito\\computational-intelligence\\Laboratories\\Lab10\\venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3501\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 3504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3505\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\repositories\\polito\\computational-intelligence\\Laboratories\\Lab10\\venv\\Lib\\site-packages\\numpy\\core\\_methods.py:118\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    115\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    116\u001b[0m         is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m ret \u001b[38;5;241m=\u001b[39m umr_sum(arr, axis, dtype, out, keepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episodes = 1_000_000\n",
    "agent = TicTacToeAgent(num_of_actions=env.action_space.n, \n",
    "                        gamma=0.98, \n",
    "                        lr=0.01, \n",
    "                        epsilon_min=0.1, \n",
    "                        epsilon_min_episode_reached=episodes)\n",
    "agent.train()\n",
    "test = False\n",
    "talk_every = 10_000\n",
    "\n",
    "ep_rewards, epl_avg = [], []\n",
    "for ep in tqdm.tqdm(range(episodes)):\n",
    "    state, info, done, steps = *env.reset(), False, 0\n",
    "    \n",
    "    agent.update_epsilon(ep)\n",
    "    turn = 0\n",
    "    sum_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        new_state, reward, done, info = env.step((action, 1))\n",
    "\n",
    "        # if done and reward > 0:\n",
    "        #     print(agent.encode_state(new_state))\n",
    "\n",
    "        if not test:\n",
    "            agent.learn(state, action, new_state, reward, done)#, talk=(done and reward>0))\n",
    "        else:\n",
    "            env.render()\n",
    "\n",
    "            \n",
    "        state = new_state\n",
    "        sum_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "    ep_rewards.append(sum_reward)\n",
    "    epl_avg.append(np.mean(ep_rewards[ max(0, ep-talk_every) : ]))\n",
    "\n",
    "    if (ep+1) % talk_every == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(\"=\"*22, \"Training resume\", \"=\"*22)\n",
    "        print(f\"Episode {ep+1}, average reward: { np.mean(ep_rewards[max(0, ep-talk_every):]) :.2f}\")\n",
    "        print(f\"Perc. wins:  {(np.sum(np.array(ep_rewards[max(0, ep-talk_every):]) > 0 )/talk_every) * 100:.4}\")\n",
    "        print(f\"Epsilon: {agent.epsilon:.4}\")\n",
    "        print(f\"Len QTable: {len(agent.q_table)}\")\n",
    "        print(\"=\"*61, end='\\r', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATE 010200000\n",
      "[ -3.861 -10.87   -6.49  -11.23   -6.152  -6.19   -5.727  -5.785  -5.324]\n",
      "False\n",
      "010212000\n",
      "[-9.82   -1.17   -9.56   -1.17   -1.17   -0.9795 -9.48   -9.42   -9.65  ]\n",
      "False\n",
      "010212010\n",
      "[-10.   0. -10.   0.   0.   0. -10.   0. -10.]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# for k,v in agent.q_table.items():\n",
    "#     # if (v > 0).any():\n",
    "#     print(k,v)\n",
    "\n",
    "state, info, done, steps = *env.reset(), False, 0\n",
    "state, reward, done, info = env.step((1,1))\n",
    "print(\"STATE\",agent.encode_state(state))\n",
    "print( agent.q_table[agent.encode_state(state)] )\n",
    "print(done)\n",
    "\n",
    "state, reward, done, info = env.step((4,1))\n",
    "print(agent.encode_state(state))\n",
    "print( agent.q_table[agent.encode_state(state)] )\n",
    "\n",
    "print(done)\n",
    "\n",
    "state, reward, done, info = env.step((7,1))\n",
    "# state, reward, done, info = env.step((8,2))\n",
    "print(agent.encode_state(state))\n",
    "print( agent.q_table[agent.encode_state(state)] )\n",
    "\n",
    "print(done)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
